{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c46e6a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3bc37cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8a9c8c",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation\n",
    "\n",
    "#### Version 0.1\n",
    "\n",
    "-----\n",
    "\n",
    "By AA Miller (Northwestern/CIERA)\n",
    "\n",
    "15 Sep 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b2bdf5",
   "metadata": {},
   "source": [
    "This notebook explores some basics for numerical maximum likelihood estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccaf88c",
   "metadata": {},
   "source": [
    "## Problem 0) Helper Functions\n",
    "\n",
    "We will need to make lots of plots of data and models throughout this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258b3051",
   "metadata": {},
   "source": [
    "**Problem 0a**\n",
    "\n",
    "Write a function `data_plot()` that creates and returns `matplotlib` figure and axes instances using [`plt.subplots`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html).\n",
    "\n",
    "The function should label the abcissa `x` and the ordinate `y`, and take optional arguments `x_obs`, `y_obs`, `y_obs_unc` that are plotted when provided by the user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12ab4e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_plot(x_obs=[], y_obs=[], y_obs_unc=[]):\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    if len(x_obs) > 0 and len(y_obs) > 0:\n",
    "        if len(y_obs_unc) > 0:\n",
    "            ax.errorbar(x_obs, y_obs, y_obs_unc, \n",
    "                        fmt='o', mec='RebeccaPurple', mfc='white', \n",
    "                        ecolor='RebeccaPurple')\n",
    "        else:\n",
    "            ax.errorbar(x_obs, y_obs, \n",
    "                        fmt='o', mec='RebeccaPurple', mfc='white')\n",
    "    \n",
    "    ax.set_xlabel('x') # complete\n",
    "    ax.set_ylabel('y') # complete\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63629bf2",
   "metadata": {},
   "source": [
    "**Problem 0b**\n",
    "\n",
    "Using `np.random` simulate 15 observations that are drawn from a linear relation defined by $f(x) = 3.14\\,x + 6.626$. The observations should be collected over the range [0,10]. \n",
    "\n",
    "Assume that the observations are noisy, and the scatter is described by a Gaussian with variance = 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65319857",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = 15 # complete\n",
    "rng = np.random.default_rng(seed=2009)\n",
    "\n",
    "x_obs = rng.uniform(0, 10, n_obs)\n",
    "y_obs = rng.normal(3.14*x_obs + 6.626, 3)\n",
    "y_obs_unc = np.ones_like(y_obs)*3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618bd2a9",
   "metadata": {},
   "source": [
    "**Problem 0c**\n",
    "\n",
    "Confirm your results from the previous two problems by using `data_plot()` to display the observations generated in **0b**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1effcf46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = data_plot(x_obs, y_obs, y_obs_unc) # complete\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f962d6e1",
   "metadata": {},
   "source": [
    "**Problem 0d**\n",
    "\n",
    "Confirm you can add to your figure instance by over-plotting the true relation used to generate the data.\n",
    "\n",
    "Did you get the Gaussian scatter correct? \n",
    "\n",
    "*Hint* – for Gaussian noise $\\sim$68% of the observations should be within 1-$\\sigma$ of the true relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37af7b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdev: 8.55160223500854\n",
      "percentage within 1-sigma: 40.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-80c74c061bd8>:4: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    }
   ],
   "source": [
    "x_obs= np.linspace(0,10,n_obs)\n",
    "yfit = 3.14*x_obs + 6.626\n",
    "ax.plot(x_obs, yfit) # complete\n",
    "fig.show()\n",
    "\n",
    "stdev = np.std(y_obs)\n",
    "print('stdev:',stdev)\n",
    "print('percentage within 1-sigma:', len(np.where(abs(yfit - y_obs) <= stdev)[0])/len(yfit)*100.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9235136e",
   "metadata": {},
   "source": [
    "*write your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7622a699",
   "metadata": {},
   "source": [
    "## Problem 1) The likelihood\n",
    "\n",
    "As we saw in Lecture II, the likelihood is just the product of the probability of all the individual observations within a data set:\n",
    "\n",
    "$$\\mathcal{L} \\equiv \\prod p(x_i|\\mathcal{M}(\\theta))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f468507c",
   "metadata": {},
   "source": [
    "Our aim is to identify model parameters $\\theta$ that *maximize* the likelihood. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa21f93",
   "metadata": {},
   "source": [
    "**Probelm 1a**\n",
    "\n",
    "Write a function `model` with two input parameters, `theta` and `x`, where `theta` is a tuple with values $\\theta_0$ and $\\theta_1$, and the function returns $\\theta_0 + \\theta_1 x$.\n",
    "\n",
    "*Hint* – this is far more formal than necessary, but it will simplify other problems later in the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6aed5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.array([1,4])*\n",
    "np.dot([1,2],[3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b10d92e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(theta, x):\n",
    "    '''\n",
    "    Return dependent variable values for f(x) = theta_0 + theta_1 x\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : tuple (size=2)\n",
    "        theta[0] is the intercept and theta[1] is the slope of the line\n",
    "    \n",
    "    x : array-like\n",
    "        values of the independent variable where f(x) should be evaluated\n",
    "    '''\n",
    "    b,m = theta\n",
    "#     result = np.dot([theta], [1,x]) # complete\n",
    "\n",
    "    return m*x+b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbeafed",
   "metadata": {},
   "source": [
    "**Problem 1b**\n",
    "\n",
    "Write a function `prob` that calculates and returns the probability of observations `x` assuming that $p(x)$ follows a normal distribution with mean `mu` and standard deviation `sigma`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8a6a156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(x, mu, sigma):\n",
    "    pi= np.pi\n",
    "    p = 1./np.sqrt(2*pi*sigma**2)*np.exp( -(x-mu)**2./(2.*sigma**2)) # complete\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0512c054",
   "metadata": {},
   "source": [
    "**Problem 1c**\n",
    "\n",
    "Calculate the likelihood for the observations that were simulated in **0b**. Use the model parameters that generated the data. \n",
    "\n",
    "*Hint* – think carefully about your variable names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5719fcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.11920572e-02 4.18925259e-12 5.73553172e-19 2.37626265e-03\n",
      " 7.13564744e-02 1.96489958e-04 5.86119787e-03 4.55608134e-02\n",
      " 1.95363895e-02 5.88936334e-10 5.62634172e-04 1.79655984e-07\n",
      " 4.45544295e-04 5.30480809e-04 7.77898858e-05]\n",
      "The likelihood for this data set is 4.1684771896468766e-74.\n"
     ]
    }
   ],
   "source": [
    "print(prob(y_obs, yfit, 3.))\n",
    "\n",
    "lkhd = np.prod(prob(y_obs, yfit, 3.))# complete\n",
    "\n",
    "print(f'The likelihood for this data set is {lkhd}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32de6f0",
   "metadata": {},
   "source": [
    "That is a very small number! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c933e1a",
   "metadata": {},
   "source": [
    "**Problem 1d**\n",
    "\n",
    "Using the same model parameters as before, generate data sets of 100, 200, and 300 observations, and calculate the likelihood for each.\n",
    "\n",
    "*Hint* – do not use the same variable names (e.g., `x_obs`, `y_obs`, etc) that we used before; we do not want to over-write those variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "983d840c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The likelihood for 100 observations is 8.260650131228858e-114\n",
      "The likelihood for 200 observations is 4.66043841730633e-218\n",
      "The likelihood for 300 observations is 0.0\n"
     ]
    }
   ],
   "source": [
    "for n_obs in [100, 200, 300]: # complete\n",
    "\n",
    "    rng = np.random.default_rng(seed=2009)\n",
    "    x_sim     = rng.uniform(0, 10, n_obs)# complete\n",
    "    y_sim     = rng.normal(3.14*x_sim + 6.626, 3)# complete\n",
    "    y_sim_unc = np.ones_like(y_sim)*3# complete\n",
    "    \n",
    "    std      = np.std(y_sim)\n",
    "    yfit_new = 3.14*x_sim + 6.626\n",
    "    lkhd = np.prod(prob(y_sim, yfit_new, np.sqrt(std)))# complete\n",
    "\n",
    "    print(f'The likelihood for {n_obs} observations is {lkhd}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decaad10",
   "metadata": {},
   "source": [
    "300 observations is a small number! Gaia has observed more than 1 billion stars, and yet, with only 300 observations in an outrageously simple dataset, the likelihood is so small it is equivalent to 0 at machine precision. \n",
    "\n",
    "\n",
    "This is why it is always a good idea to work with the log of the likelihood, not only does this turn a product into a sum, more importantly, the calculations become far more stable on your machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd49aa5",
   "metadata": {},
   "source": [
    "**Problem 1e**\n",
    "\n",
    "Write a function `lnl` that calculates the log likelihood for some observations, their uncertainties, and the model to which the observations are being compared. Assume that the likelihood is Gaussian.\n",
    "\n",
    "*Hint* – the `model` function that was created earlier should be inside the `lnl` function, this means that `theta` should be the first arguement for the `lnl` function.\n",
    "\n",
    "*Note* – likelihoods are calculated for comparison purposes, their absolute value does not have much meaning, so you can ignore constant terms for this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5fd02b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnl(theta, y_obs, y_obs_unc, x_obs):\n",
    "    # complete\n",
    "    pi = np.pi\n",
    "    mu = model(theta, x_obs)\n",
    "    xi = y_obs\n",
    "    si = y_obs_unc #np.sqrt(y_obs)\n",
    "    N  = len(y_obs)\n",
    "    z  = (y_obs - mu)/si\n",
    "    result = -1/2*np.sum(z**2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42537ae2",
   "metadata": {},
   "source": [
    "**Problem 1f**\n",
    "\n",
    "Using the same model parameters as before, generate data sets of 100, 200, and 300 observations, and calculate the *log likelihood* for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1864cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ln likelihood for 100 observations is -58.68946115574463\n",
      "The ln likelihood for 200 observations is -96.81356610831219\n",
      "The ln likelihood for 300 observations is -148.61949734946114\n"
     ]
    }
   ],
   "source": [
    "for n_obs in [100, 200, 300]:\n",
    "\n",
    "    rng = np.random.default_rng(seed=2009)\n",
    "\n",
    "    x_sim = rng.uniform(0, 10, n_obs)\n",
    "    y_sim = rng.normal(3.14*x_sim + 6.626, 3)\n",
    "    y_sim_unc = np.ones_like(x_sim)*3\n",
    "    THETA  = (6.626, 3.14)\n",
    "    lnlike = lnl(THETA, y_sim, y_sim_unc, x_sim)# complete\n",
    "\n",
    "    print(f'The ln likelihood for {n_obs} observations is {lnlike}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9387319f",
   "metadata": {},
   "source": [
    "## Problem 2) Maximizing the Likelihood\n",
    "\n",
    "It is all well and good to calculate the likelihood, but what we truly want is to maximize the likelihood. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf89bbbf",
   "metadata": {},
   "source": [
    "Most algorithms are designed to minimize, rather than optimize, a function. Fortunately, minimizing the negative log likelihood is the exact same as maximizing the log likelihood.\n",
    "\n",
    "**Problem 2a**\n",
    "\n",
    "Write a function `nll` to calculate the negative log likelihood. \n",
    "\n",
    "*Hint* – this is really simple, don't overthink it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5fc4174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(theta, y_obs, y_obs_unc, x_obs): #complete\n",
    "    return -lnl(theta, y_obs, y_obs_unc, x_obs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14819a1c",
   "metadata": {},
   "source": [
    "**Problem 2b**\n",
    "\n",
    "Using `minimize` from `scipy.optimize` determine the maximum likelihood estimation for the intercept and slope of the line that was used to generate the synthetic observations. \n",
    "\n",
    "*Hint* – for arguments `minimize` needs (1) a function, (2) an initial guess for the model parameters, which is why we've been using a tuple `theta`, and (3) a tuple containing the remaining arguments for the function to be minimized (i.e., the data/observations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4fcc2d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b = 21.1546 and m = 0.0277 for the MLE\n"
     ]
    }
   ],
   "source": [
    "res = minimize(nll, (1,1), (y_obs, y_obs_unc, x_obs) ) # complete\n",
    "\n",
    "print(f'b = {res.x[0]:.4f} and m = {res.x[1]:.4f} for the MLE')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ee19e8",
   "metadata": {},
   "source": [
    "**Problem 2c**\n",
    "\n",
    "Overplot the line determined by the MLE on top of the synthetic data. \n",
    "\n",
    "How does the line compare to the true line used to generate the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7551ed46",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-abf321dffcd4>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-abf321dffcd4>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    fig.tight_layout()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "fig, ax = data_plot(x_obs, y_obs, y_obs_unc)\n",
    "ax.plot( # complete\n",
    "ax.plot( # complete\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de06bfb2",
   "metadata": {},
   "source": [
    "## Problem 3) Point estimates are nice, but...\n",
    "\n",
    "In lecture we did not cover how to estimate uncertainties on the model parameters. Using some analytical approximations to the shape of the likelihood it is possible to estimate these values. Using something like `curve_fit` in `scipy` this is where the uncertainties come from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e3489e",
   "metadata": {},
   "source": [
    "These methods require that the data do not violate any of the underlying assumptions. For this reason, I prefer to use the bootstrap method to estimate uncertainties on the MLE model parameters. \n",
    "\n",
    "Briefly, \"new\" data sets can be simulated via a bootstrap (randomly choosing a new data set from the observations with replacement) and then finding the MLE parameters again. This can be repeated many times to place confidence limits on the MLE estimates. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f066bc76",
   "metadata": {},
   "source": [
    "**Problem 3a**\n",
    "\n",
    "Using [np.random.choice](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html) select a bootstrap sample from the observations and estimate the MLE model parameters. \n",
    "\n",
    "*Hint* – choose a random index so that the input x, y, and y_unc samples are all the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc132681",
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_idx = np.random.choice( # complete\n",
    "\n",
    "res = minimize(nll, (0,0), # complete\n",
    "\n",
    "print(f'b = {res.x[0]:.4f} and m = {res.x[1]:.4f} for the MLE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cd4e7d",
   "metadata": {},
   "source": [
    "**Problem 3b**\n",
    "\n",
    "Create 100 bootstrap samples of the simulated observations and measure the MLE intercept and slope for each. \n",
    "\n",
    "Store the results in arrays called `b_boot` and `m_boot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cfc7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_boot = 100\n",
    "b_boot = np.zeros(n_boot)\n",
    "m_boot = np.zeros_like(b_boot)\n",
    "\n",
    "for idx in range(n_boot):\n",
    "    boot_idx = # complete\n",
    "    res = # complete\n",
    "    b_boot[idx] = # complete\n",
    "    m_boot[idx] = # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bf19f4",
   "metadata": {},
   "source": [
    "**Problem 3c**\n",
    "\n",
    "The 68% confidence interval on $b$ (or $m$) is just the central 68% of the bootstrap measuerd distribution for the parameter.\n",
    "\n",
    "Use `np.percentile()` to determine the 68% C.I. on $b$ and $m$. Do these estimates bracket the true values used to generate the data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646b07e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The 68% C.I. on b is {:.4f} to {:.4f}'.format(*np.percentile(b_boot, # complete\n",
    "\n",
    "print('The 68% C.I. on m is {:.4f} to {:.4f}'.format(*np.percentile(m_boot, # complete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef65b258",
   "metadata": {},
   "source": [
    "*write your answer here*\n",
    "\n",
    "The bootstrap estimated C.I. for both parameters does surround the true answers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee4a5f",
   "metadata": {},
   "source": [
    "**Problem 3d**\n",
    "\n",
    "What about correlations between the model parameters? \n",
    "\n",
    "Plot the 2d histogram of `m_boot` and `b_boot` to see if the estimates are correlated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bf4161",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "cax = ax.hist2d( # complete\n",
    "fig.colorbar(cax[3])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96c7bc6",
   "metadata": {},
   "source": [
    "## Problem 4) Model Selection\n",
    "\n",
    "During lecture we saw two potential methods for selecting between different models. The first was using the $\\chi^2_\\mathrm{dof}$ and the second was AIC. We will briefly examine each below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed75442f",
   "metadata": {},
   "source": [
    "**Problem 4a** \n",
    "\n",
    "Plot the simulated data in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b16c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = 23\n",
    "rng = np.random.default_rng(seed=1851)\n",
    "x_obs = rng.uniform(0, 10, n_obs)\n",
    "y_obs = rng.normal(7.89*np.sin(2*np.pi*x_obs/23) + 0.4*x_obs, 1)\n",
    "y_obs_unc = np.ones_like(y_obs)*1\n",
    "\n",
    "fig, ax = data_plot(x_obs, y_obs, y_obs_unc)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a4d2e",
   "metadata": {},
   "source": [
    "**Problem 4b**\n",
    "\n",
    "Modify the function `model` from **1a** to accept a tuple of any length and return a polynomial of degree `len(theta)`-1. (This will allow us to compare polynomials of different degree in these problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b22c417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(theta, x):\n",
    "    '''\n",
    "    Return dependent variable values for polynomial\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : tuple\n",
    "        coefficients for a polynomial of degree len(theta)-1\n",
    "    \n",
    "    x : array-like\n",
    "        values of the independent variable where f(x) should be evaluated\n",
    "    '''\n",
    "    \n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    \n",
    "    return # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4fc7b9",
   "metadata": {},
   "source": [
    "**Problem 4c**\n",
    "\n",
    "Write a function `chi2dof` to calculate the $\\chi^2_\\mathrm{dof}$ for a polynomial model. The inputs should be `theta`, as well as the observations and their uncertainties. \n",
    "\n",
    "*Hint* – the solution will be dependent upon functions created in the previous problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cd7a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2dof(theta, y_obs, y_obs_unc, x_obs): \n",
    "    \n",
    "    res = # complete\n",
    "\n",
    "    dof = # complete\n",
    "    \n",
    "    chi2 = # complete\n",
    "    \n",
    "    return chi2/dof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789b2bfd",
   "metadata": {},
   "source": [
    "**Problem 4d**\n",
    "\n",
    "Fit a first, second, and fifth order polynomial to the observations. \n",
    "\n",
    "Which model is \"best\" based on the $\\chi^2_\\mathrm{dof}$?\n",
    "\n",
    "Bonus points - overplot the MLE models on top of the data (you may need to modify `chi2dof` in order to do this easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c8f88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for order in [1, 2, 5]:\n",
    "    # complete\n",
    "    print('The chi2 for {}th degree polynomial is: {}'.format( # complete\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e8a87d",
   "metadata": {},
   "source": [
    "*write your response here*\n",
    "\n",
    "For this particular data set the 2nd degree polynomial provides the best match to the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdbada1",
   "metadata": {},
   "source": [
    "## Problem 5) Numerical stability\n",
    "\n",
    "Polynomials are often used for examples on numerical methods, but nature often provides more complex relations (exponentials, periodicity, power-laws, etc) between the quantities that we can measure. We will examine how that affects MLE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f0b4b7",
   "metadata": {},
   "source": [
    "**Problem 5a**\n",
    "\n",
    "Execute the cell below to simulate an exponentially declining signal that has a small \"bump\" at $x = 5$:\n",
    "\n",
    "$$f(x) = A \\exp(-x) - B\\;(x - C)^2 \\exp(-(x - C)^2)$$\n",
    "\n",
    "where for this problem $A = 8$, $B = 15$ and $C = 5$. \n",
    "\n",
    "*Note* – there is no specific physical system that inspires this data, but you can find examples that look somewhat like this (e.g. emission line in spectra or resonance line in high-energy particle detectors). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60a178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = 55\n",
    "rng = np.random.default_rng(seed=1851)\n",
    "x_obs = rng.uniform(0, 10, n_obs)\n",
    "y_obs = rng.normal(8*np.exp(-x_obs) - 15*(x_obs-5)**2*np.exp(-(x_obs-5)**2), 1)\n",
    "y_obs_unc = np.ones_like(y_obs)*1\n",
    "\n",
    "fig, ax = data_plot(x_obs, y_obs, y_obs_unc)\n",
    "# ax.set_yscale('log')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2086842c",
   "metadata": {},
   "source": [
    "**Problem 5b**\n",
    "\n",
    "Modify the function `model` to fit the declining exponential function $f(x)$ given in **5a**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a563f8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(theta, x):\n",
    "    '''\n",
    "    Return dependent variable values for polynomial\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : tuple (size = 3)\n",
    "        A, B, C values for the exponential function A np.exp(-x) - B*(x - C)**2 * np.exp(-(x - C)**2)\n",
    "    \n",
    "    x : array-like\n",
    "        values of the independent variable where f(x) should be evaluated\n",
    "    '''\n",
    "    \n",
    "    # complete\n",
    "        \n",
    "    \n",
    "    return # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb55174",
   "metadata": {},
   "source": [
    "**Problem 5c**\n",
    "\n",
    "Find the maximum likelihood solution given the observations `y_obs` and `y_obs_unc`. Use an intial guess in your call to `nll` of (1, 1, 1). \n",
    "\n",
    "Are your results close to the correct answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b50b008",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = # complete\n",
    "\n",
    "print('A = {:.4f}, B = {:.4f}, C = {:.4f}'.format(*res.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25ab228",
   "metadata": {},
   "source": [
    "*write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1b5081",
   "metadata": {},
   "source": [
    "**Problem 5d**\n",
    "\n",
    "Find the maximum likelihood solution given the observations `y_obs` and `y_obs_unc`. Use an intial guess in your call to `nll` of (20, 20, 20). \n",
    "\n",
    "Are your results close to the correct answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e47b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = # complete\n",
    "\n",
    "print('A = {:.4f}, B = {:.4f}, C = {:.4f}'.format(*res.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137e3748",
   "metadata": {},
   "source": [
    "*write your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40def175",
   "metadata": {},
   "source": [
    "Here we have identified one of the problems with MLE (though to be fair, there are similar issues if you are conducting a Bayesian analysis): there is no guarantee that the minimization algorithm that is being used will identify a global minimum. The results that the algorithm obtains can be highly dependent upon the starting position for the minimizer or even the method used to perform the minimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d31892",
   "metadata": {},
   "source": [
    "**Problem 5e**\n",
    "\n",
    "Consult the docs for `minimize` and adjust parameters relative to the defaults, but the same starting point as **5d** (20,20,20), to arrive at a better solution for the values of $A$, $B$, and $C$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cc8a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = # complete\n",
    "\n",
    "print('A = {:.4f}, B = {:.4f}, C = {:.4f}'.format(*res.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e3374b",
   "metadata": {},
   "source": [
    "## Challenge Problem) Improved Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a24cee1",
   "metadata": {},
   "source": [
    "In lecture we saw that the AIC could be used as a model selection technique. \n",
    "\n",
    "As a reminder: \n",
    "\n",
    "$$\\mathrm{AIC} \\equiv -2 \\ln \\mathcal{L} + 2k + \\frac{2k(k+1)}{N - k -1},$$\n",
    "\n",
    "where $N$ is the number of observations, $k$ is the number of model parameters, and models with the smallest AIC are preffered. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164d6f33",
   "metadata": {},
   "source": [
    "**Challenge 0a**\n",
    "\n",
    "For the data simulated in the previous problem – calculate the AIC for polynomials of degree 1 through 10. \n",
    "\n",
    "Plot the AIC as a function of polynomial degree. According to the AIC, which model best replicates the observations? \n",
    "\n",
    "Do you notice anything interesting about your results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1049dba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
